\documentclass[10pt]{article}
\usepackage{fullpage}
%the bm package is for clean looking bold math symbols
%in math mode, use the command \bm{<math>} to make
%something bold.
\usepackage{color, amssymb, amsmath, textcomp, bm}
\usepackage{rotating}
%These are the graphics packages
\usepackage{graphicx, graphics, epsfig}
\usepackage{multicol}

%These are new commands that I've created
%The format is {<new command>}{{what the command does}}
%good for shortening commands you commonly use.
\newcommand{\tr}{{\mathrm{tr}}}
\newcommand{\rmk}{{\mathcal{K}}}
\newcommand{\I}{{\mathrm{I}}}
\newcommand{\E}{{\mathrm{E}}}
\newcommand{\B}{{\mathcal{B}}}
\newcommand{\cov}{{\mathrm{Cov}}}
\newcommand{\vs}{{\vspace{.07in}}}
\newcommand{\hs}{{\hspace{.05in}}}
\newcommand{\n}{{\newline}}
\newcommand{\ARMA}{{\mathrm{ARMA}}}
\newcommand{\N}{{\mathrm{N}}}

%\newcommand{\exp}{{{\mathrm{exp}}}} already defined

%similar to to newcommand, but shortens some
%theorem and section headings
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{defi}{Definition}[section]
\newtheorem{example}{Example}[section]
\pagestyle{plain}
%\def{\ni}{\noindent}


\author{Matt Simpson}
\title{Sanity Check for Iterative Simple Linear Regression}

\begin{document}

\section{Introduction}

One idea to avoid doing matrix arithmetic
in the model selection algorithm, suggested by Dan Nettleton is to
avoid computing the coefficient estimates altoether. In order to do
this, we first fit all simple linear regression models. Then to fit
regression models with two variables included, we find one of the SLR
models with only one of the two desired covariate then regress that
model's residuals on the new covariate, yielding the residuals of the
larger model. In this fashion, we can start from the smallest possible
models and iteratively obtain the residuals of every possible model
using only simple linear regressions - i.e. avoiding all matrix
calculations. This procedure doesn't seem to work, as I understand
it. The \verb0R0 file in this directory, \verb0sanitycheck.r0,
provides a counterexample. The next section fleshes out how I
understand the process is supposed to work and the following section
is a bit of theory showing that it shouldn't work.

\section{Iterative SLR}

Suppose we want to obtain the residuals from a full regression model
\begin{align}\label{full}
\bm{y} = \bm{X}\bm{\beta} + \bm{\epsilon}
\end{align}
without actually fitting the model. Break up $\bm{X}$ and $\bm{\beta}$
into
\[
\bm{X} = \begin{bmatrix} \bm{X_1}, & \bm{X_2} \\\end{bmatrix}
\ and\
\bm{\beta} = \begin{bmatrix} \bm{\beta_1} \\ \bm{\beta_2} \end{bmatrix}
\]
so that
\[
\bm{y} = \bm{X_1}\bm{\beta_1} + \bm{X_2}\bm{\beta_2} + \bm{\epsilon}
\]
Rearranging yields
\begin{align}\label{full2}
\bm{y} - \bm{X_1}\bm{\beta_1} = \bm{X_2}\bm{\beta_2} + \bm{\epsilon}
\end{align}
Note that the left hand side is just the residual from the model
\begin{align}\label{red}
\bm{y} = \bm{X_1}\bm{\beta_1} + \bm{\delta}
\end{align}
so that we can rewrite \eqref{full2} as
\begin{align}\label{red2}
\bm{\delta} = \bm{X_2}\bm{\beta_2} + \bm{\epsilon}
\end{align}
Thus, the argument goes, if we first fit model \eqref{red} then fit
model \eqref{red2} using the residuals from \eqref{red}, we can obtain
the residuals to the full model \eqref{full}. As long as $\bm{X_2}$ is
a single column, then we only ever have to fit simple linear
regression models and most of those without an intercept.

\section{Residual Relationship}
The example in \verb0sanitycheck.r0 already shows that this basic
process doesn't work as expected, but I'll show why here. First, some
preliminaries. Suppose we have the following regression model
\[
\bm{y} = \bm{X}\bm{\beta} + \bm{\epsilon}
\]
The least squares estimate for $\bm{\beta}$ is
\[
\bm{\hat{\beta}} = \left(\bm{X}'\bm{X}\right)^{-1}\bm{X}'\bm{y}
\]
which yields the estimated residuals
\[
\bm{\hat{\epsilon}} = \bm{y} - \bm{\hat{y}} = \bm{y} -
\bm{X}\bm{\hat{\beta}} = \bm{y} -
\bm{X}\left(\bm{X}'\bm{X}\right)^{-1}\bm{X}'\bm{y} = \bm{y} -
\bm{P_X}\bm{y} = \left(\bm{I} - \bm{P_X}\right)\bm{y}
\]
where $\bm{P_X} = \bm{X}\left(\bm{X}'\bm{X}\right)^{-1}\bm{X}'$ is the
projection matrix into the column space of $\bm{X}$.

So the estimated residuals for model \eqref{full} -- the full model -- are
\[
\bm{\hat{\epsilon}} = (\bm{I} - \bm{P_X})\bm{y}
\]
the estimated residuals for model \eqref{red} -- the
reduced model -- are
\[
\bm{\hat{\delta}} = (\bm{I} - \bm{P_{X_1}})\bm{y}
\]
and the estimated residuals for model \eqref{red2} -- the residual
model -- are
\[
\bm{\hat{\eta}} = (\bm{I} - \bm{P_{X_2}})\bm{\hat{\delta}} = (\bm{I} - \bm{P_{X_2}})(\bm{I} - \bm{P_{X_1}})\bm{y}
\]

In order for the process outlined in section 2 to work, we need
$\bm{\hat{\epsilon}} = \bm{\hat{\eta}}$, or
\[
(\bm{I} - \bm{P_X})\bm{y} = (\bm{I} - \bm{P_{X_2}})(\bm{I} - \bm{P_{X_1}})\bm{y}
\]
where $\bm{X} = \begin{bmatrix} \bm{X_1}, &
  \bm{X_2} \end{bmatrix}$. We could still salvage the idea if
$\bm{\hat{\epsilon}} = f(\bm{\hat{\eta}})$ or $\bm{\hat{\epsilon}} =
f(\bm{\hat{\delta}})$ where $f$ is some easily computable function.

I'll show that $\bm{\hat{\epsilon}} = f(\bm{\hat{\delta}},
\bm{P_{X_1}})$, that is the residuals of the full model is a function
of the residuals of the reduced model {\it and} the projection
matrix of the reduced model. This function won't be useful to us
because we still have to fit the reduced model at every step in order
to obtain the projection matrix of the reduced models and thus the
residuals of the next larger model. I won't show that there isn't a
function mapping the residuals of the reduced model onto the residuals
of the full model that doesn't depend on $\bm{P_{X_1}}$, but the
function I do derive suggests that such a function doesn't exist.

Start with the full model with the model matrix split up
\[
\bm{y} = \begin{bmatrix} \bm{X_1}, &
  \bm{X_2} \end{bmatrix} \begin{bmatrix} \bm{\beta_1} \\
  \bm{\beta_2} \end{bmatrix} + \bm{\epsilon}
\]
The we can rewrite $\bm{\hat{\beta}}$ as
\begin{align}\label{betahat}
\bm{\hat{\beta}} = \left(\begin{bmatrix} \bm{X_1}' \\
    \bm{X_2}' \end{bmatrix} \begin{bmatrix}
  \bm{X_1}, & \bm{X_2} \end{bmatrix} \right)^{-1} \begin{bmatrix}
\bm{X_1}' \\ \bm{X_2}' \end{bmatrix}\bm{y}
= \begin{bmatrix} \bm{X_1}'\bm{X_1} & \bm{X_1}'\bm{X_2} \\
  \bm{X_2}'\bm{X_1} & \bm{X_2}'\bm{X_2} \end{bmatrix}^{-1}
\begin{bmatrix} \bm{X_1}'\bm{y} \\ \bm{X_2}'\bm{y} \end{bmatrix}
\end{align}

We can find the inverse in \eqref{betahat} using the following known formula:
\[
\begin{bmatrix} \bm{A} & \bm{B} \\ \bm{C} & \bm{D} \end{bmatrix} ^{-1} =
\begin{bmatrix} \bm{A}^{-1} + \bm{A}^{-1}\bm{B}\bm{S}\bm{C}\bm{A}_{-1}
  & -\bm{A}^{-1}\bm{B}\bm{S} \\
  -\bm{S}\bm{C}\bm{A}^{-1} & \bm{S} \end{bmatrix}
\]
where $\bm{S} = (\bm{D} - \bm{C}\bm{A}^{-1}\bm{B})^{-1}$ is the
inverse Schur complement of $\bm{A}$. In this case
\[
\bm{S} = \left(\bm{X_2}'\bm{X_2} -
  \bm{X_2}'\bm{X_1}(\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}'\bm{X_2}\right)^{-1}
= (\bm{X_2}'\bm{X_2} - \bm{X_2}'\bm{P_{X_1}}\bm{X_2})^{-1} =
\left[\bm{X_2}'(\bm{I} - \bm{P_{X_1}})\bm{X_2}\right]^{-1}
\]
and
\begin{align*}
\bm{\hat{\beta}} &= \widehat{\begin{bmatrix} \bm{\beta_1} \\
    \bm{\beta_2} \end{bmatrix}} =
\begin{bmatrix} (\bm{X_1}'\bm{X_1})^{-1} +
  (\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}'\bm{X_2} \bm{S}
  \bm{X_2}'\bm{X_1}(\bm{X_1}'\bm{X_1})^{-1} &
  -(\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}'\bm{X_2}\bm{S} \\
  -\bm{S}\bm{X_2}'\bm{X_1}(\bm{X_1}'\bm{X_1})^{-1} &
  \bm{S} \end{bmatrix}
\begin{bmatrix} \bm{X_1}'\bm{y}\\ \bm{X_2}'\bm{y}\end{bmatrix} \\
&= \begin{bmatrix} (\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}\bm{y} +
  (\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}'\bm{X_2}\bm{S}\bm{X_2}'\bm{P_{X_1}}\bm{y}
  - (\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}'\bm{X_2}\bm{S}\bm{X_2}'\bm{y} \\
  -\bm{S}\bm{X_2}'\bm{P_{X_1}}\bm{y} +
    \bm{S}\bm{X_2}'\bm{y} \end{bmatrix}\\
&=\begin{bmatrix} (\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}'\bm{y} -
  (\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}'\bm{X_2}\bm{S}\bm{X_2}'\bm{\hat{\delta}}\\
  \bm{S}\bm{X_2}'\bm{\hat{\delta}} \end{bmatrix}
\end{align*}
This yields
\begin{align*}
\bm{\hat{y}} &= \bm{X}\bm{\hat{\beta}} = \begin{bmatrix} \bm{X_1} &
  \bm{X_2} \end{bmatrix} \bm{\hat{\beta}} = \bm{P_{X_1}}\bm{y} -
\bm{P_{X_1}}\bm{X_2}\bm{S}\bm{X_2}'\bm{\hat{\delta}} +
\bm{X_2}\bm{S}\bm{X_2}'\bm{\hat{\delta}} \\
&= \bm{P_{X_1}}\bm{y} +
(\bm{I}-\bm{P_{X_1}})\bm{X_2}\bm{S}\bm{X_2}'\bm{\hat{\delta}}
\end{align*}
So
\begin{align*}
\bm{\hat{\epsilon}} &= \bm{y}-\bm{\hat{y}} = \bm{\hat{\delta}} -
(\bm{I}-\bm{P_{X_1}})\bm{X_2}\left[\bm{X_2}'(\bm{I}-\bm{P_{X_1}})\bm{X_2}\right]^{-1}\bm{\hat{\delta}}
\end{align*}

To restate the result just show, let the full regression model be of
the form
\[
\bm{y} = \bm{X}\bm{\beta} + \bm{\epsilon}
\]
where $\bm{X} = \begin{bmatrix} \bm{X_1} &
  \bm{X_2} \end{bmatrix}$. Then let the reduced model be of the form
\[
\bm{y} = \bm{X_1}\bm{\beta_1} + \bm{\delta}
\]
Then the estimated residuals of the full model are related to the
estimated residuals of the reduced model in the following manner:
\[
\bm{\hat{\epsilon}} = \bm{\hat{\delta}} -
(\bm{I}-\bm{P_{X_1}})\bm{X_2}\left[\bm{X_2}'(\bm{I}-\bm{P_{X_1}})\bm{X_2}\right]^{-1}\bm{\hat{\delta}}
\]
So $\bm{\hat{\epsilon}}$ still depends on the projection matrix of the
reduced model, $\bm{P_{X_1}}$. This still may be useful, however, if
the equation can be manipulated so that the projection matrices can be
turned into the reduced model residuals. Note, however, that if the
equation still requires matrix multiplication even when $\bm{X_2}$ is
a single column, this method may not yield any gains. At the moment,
nothing seems possible. Note that $\bm{y}\bm{y}'$ is not invertible,
so simply inserting $\bm{y}\bm{y}'(\bm{y}\bm{y}')^{-1}$ into the
equation strategically will not work.
\end{document}