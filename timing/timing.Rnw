\documentclass{article}
\usepackage{graphicx, color, amssymb, amsmath, bm, rotating, graphics,
epsfig, multicol}
\usepackage{cite}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}
\begin{document}

% \SweaveOpts{fig.path='figure/', fig.align='center', fig.show='hold'}

<<setup, include=FALSE, cache=FALSE>>=
options(replace.assign=TRUE,width=90)
@


\title{End of Summer Summary}


\author{Matt Simpson}

\maketitle
\date

\section{Introduction}

The goal for the summer was to explore using massively parallel
computation on the GPU in order to speed up model selection
computation in the linear model, specifically, to speed up covariate
selection for regression. Section 2 formally states the computational
problem that we are trying to speed up and the existing tools for
doing this on a GPU. Section 3 details the methods I used to implement
and improve upon existing tools. Section 4 provides some details on a
bug I discoverd in the CUBLAS library and what it means for this
project. Section 5 shows the results of timing the various methods and
section 6 explores avenues for further work, including an iterative
simple linear regression method suggested by Dan Nettleton.

\section{Problem Statement}
Suppose we have a continuous variable, $y$, and a vector of
covariates, $\bm{x}$, such that we know some elements of $\bm{x}$ are
related to $y$ but most are not. The problem is to choose a subset of
$\bm{x}$ which provides the best fit or prediction of $y$ for some
sense of the word ``best.''

Formally, suppose we have an $n\times1$ response vector, $\bm{y} =
(y_1,,...,y_n)'$ that is normally distributed with mean $\bm{\mu}
= (\mu_1,...\mu_n)'$ and covariance $\sigma^2\bm{I}_n$ where
$\sigma^2$ is the variance parameter and $\bm{I}_n$ is the $n\times n$
identity matrix. We have a set of $k$ $n\times1$ predictors
$\bm{x}_1,...\bm{x}_k$ and asssume that the mean vector $\bm{\mu}$ is
in the span of $\bm{1}_n,\bm{x}_1,...,\bm{x}_k$. The model selection
problem is to select a subset of predictor variables thereby placing
additional restrictions on the mean, $\bm{\mu}$. Let the model space
be indexed by $\bm{\gamma}=(\gamma_1,...,\gamma_k)'$ where
$\gamma_i=1$ indicates that $\bm{x}_i$ is an included predector
variable for model $\bm{\gamma}$ and $\gamma_i=0$ indicates that
$\bm{x}_i$ is an excluded predictor variable. Thus under model
$\bm{\gamma}$, the mean can be written as
\[
\bm{\gamma}: \bm{\mu} = \alpha_{\bm{\gamma}} +
\bm{X}_{\bm{\gamma}}\bm{\beta}_{\bm{\gamma}}
\]
where $\bm{X}_{\bm{\gamma}}$ is the $n\times k_{\bm{\gamma}}$ design
matrix for model $\bm{\gamma}$ that includes $k_{\bm{\gamma}}$
$n\times 1$ columns of predictor variables and
$\bm{\beta}_{\bm{\gamma}}$ is the $k_{\bm{\gamma}}\times1$ vector of
nonzero coefficients for model $\bm{\gamma}$, while
$\alpha_{\bm{\gamma}}$ is the intercept for model $\bm{\gamma}$. An
alternative way of representing each model is
\[
\bm{\gamma} : \bm{y} = \alpha_{\bm{\gamma}} +
\bm{X_\gamma}\bm{\beta_\gamma} + \bm{\epsilon_\gamma}
\]
with the errors $\bm{\epsilon_\gamma}$ distributed
$\mathrm{N}(\bm{0}_n,\sigma^2_{\bm{\gamma}}\bm{I}_n)$.

\subsection{Model Fitting}\label{fit}
The standard least squares estimate for
$\bm{\lambda_\gamma}=(\alpha_{\bm{\gamma}},\bm{\beta_\gamma}')'$ is
$\hat{\bm{\lambda_\gamma}} =
(\bm{Z_\gamma}'\bm{Z_\gamma})^{-1}\bm{Z_\gamma}'\bm{y}$ where
$\bm{Z_\gamma}=\left[\bm{1}_n,\bm{X_\gamma}\right]$ while the
standard estmate for the error variance $\sigma^2_{\bm{\gamma}}$ is
$\hat{\sigma}^2_{\bm{\gamma}} = \frac{SSE}{n-p_{\bm{\gamma}}}$ where
$SSE = (\bm{y}-\bm{Z_\gamma}\hat{\bm{\lambda}}_{\bm{\gamma}})'
(\bm{y}-\bm{Z_\gamma}\hat{\bm{\lambda}}_{\bm{\gamma}})$ and
$p_{\bm{\gamma}} = k_{\bm{\gamma}} + 1$.

In a Bayesian context, the regression model also requires priors on
$\alpha_{\bm{\gamma}}$, $\bm{\beta_\gamma}$, and
$\sigma^2_{\bm{\gamma}}$. The standard noninformative prior for model
selection is Zellner's g-prior:
\begin{align*}
  p(\alpha,\sigma^2|\bm{\gamma})\ &=\ \sigma^2\\
  \bm{\beta_\gamma}|\sigma^2,\bm{\gamma}\ &\sim\
  \mathrm{N}\left(g\sigma^2(\bm{X_\gamma}'\bm{X_\gamma})^{-1}\right)
\end{align*}
for some choice of $g$ \cite{Liang2008mixtures}. Alternatively, we
can put a prior on $g$ as Liang et. al. \cite{Liang2008mixtures}
suggest, but we'll assume a particular value of $g$ has been chosen
for simplicity. From these priors, the posterior distribution of
$(\sigma^2_{\bm{\gamma}},\alpha_{\bm{\gamma}},\bm{\beta_\gamma})$ can
be derived. This distribution, incidentally, depends on
$\hat{\bm{\lambda}}_{\bm{\gamma}}$ and $\hat{\sigma}^2_{\bm{\gamma}}$.

\subsection{Model Selection}

There are several different methods for choosing the best model or
subset of models for the mean. Some of these include AIC, BIC, and
Bayes Factors.

\subsubsection{AIC}

AIC, or Akike's Information Criterion, is a measure of a model's
fit. For the linear regression model with normal errors, AIC is, up to
a positive, additive constant
\[
AIC(\bm{\gamma}) =
n\log\left(\hat{\sigma}^2_{\bm{\gamma}}\right) +2(p_{\bm{\gamma}}+1)
\]

Thus in order to determince AIC for a given regression model, the only
information needed from the fitting process is the estimated
variance. Only relative differences in AIC are informative so that no
particular value of AIC indicates a ``good'' fit, but models with
smaller AIC have better fit than models with larger AIC.

\subsubsection{BIC}

BIC or the Bayesian information criterion is another measure of a
model's fit. For the normal error linear regression model, BIC is
determined up to a positive, additive constant as

\[
BIC(\bm{\gamma}) =
n\log\left(\hat{\sigma}^2_{\bm{\gamma}}\right) + (p_{\bm{\gamma}}+1)\log(n)
\]

Much like AIC, the only information needed from the fitting process to
determine BIC is the estimate variance. Interpretation of BIC is
exactly the same as that of AIC -- models with smaller values of BIC
have a better fit than models with larger values, but the value of BIC
gives no absolute information about whether any of the class of models
fits well. BIC tends to be more conservative than AIC, selecting
models with less covariates.


\subsubsection{Bayes Factors}

A fully Bayesian approach to model selection is the Bayes factor. In
general, the Bayes factor for comparing two models is
\begin{align*}
  BF(\bm{\gamma}_1,\bm{\gamma}_1)\ &=\
  \frac{p(\bm{y}|\bm{\gamma}_1)}{p(\bm{y}|\bm{\gamma}_2)}\\
\end{align*}
where $\bm{y}$ is the data, $p(\bm{y}|\bm{\gamma})$ is the marginal
likelihood, defined as
\begin{align*}
  p(\bm{y}|\bm{\gamma})\ &=\ \int
  p(\bm{y}|\bm{\theta},\bm{\gamma})\pi(\bm{\theta}|\bm{\gamma})d\bm{\theta}
\end{align*}
such that $\bm{\theta}$ is the vector of paramters and
$\pi(\bm{\theta}|\bm{\gamma})$ is the prior on $\bm{\theta}$
conditional on model $\bm{\gamma}$. The Bayes factor quantifies the
evidence in favor of model 1 over model 2. $BF=1$ indicates that the
data doesn't support one model over the other while $BF>1$ means that
the data favor model 1. Given $M$ possible models and a
prior $p(\bm{\gamma})$ over all posible models, the posterior
probability of each model can be calculated as
\begin{align*}
  p(\bm{\gamma}|\bm{y}) =
  \frac{p(\bm{\gamma})p(\bm{y}|\bm{\gamma})}{\sum_{\bm{\gamma}}
    p(\bm{\gamma})p(\bm{y}|\bm{\gamma})}
\end{align*}
The posterior odds in favor of model 1 over model 2,
$\frac{p(\bm{\gamma}_1|\bm{y})}{p(\bm{\gamma}_2|\bm{y})}$ can be
written as
\[
posterior\ odds\ =\ Bayes\ factor\ \times\ prior\ odds
\]
and similarly posterior model probabilities can be calculated from
Bayes factors, given some base model $\bm{b}$:
\begin{align*}
  p(\bm{\gamma}|\bm{y})=\frac{p(\bm{\gamma})BF(\bm{\gamma},\bm{b})}
  {\sum_{\bm{\gamma}}p(\bm{\gamma})BF(\bm{\gamma},\bm{b})}
\end{align*}

Key in these computations is the marginal likelihood,
$p(\bm{y}|\bm{\gamma})$. It turns out that given Zellner's g-prior and
a chosen value for $g$, the marginal likelihood for the regression
model can be derived analytically as \cite{Liang2008mixtures}
\begin{align*}
  p(\bm{y}|\bm{\gamma},g)\ &=\
  \frac{\Gamma\left(\frac{n-1}{2}\right)}
  {\sqrt{\pi}^{(n-1)}\sqrt{n}}||\bm{y}-\bar{\bm{y}}||^{-(n-1)}
  \frac{(1+g)^{(n-1-k_{\bm{\gamma}})/2}}{[1+g(1-R^2_{\bm{\gamma}})]^{(n-1)/2}}\\
  &\propto \frac{(1+g)^{(n-1-k_{\bm{\gamma}})/2}}{[1+g(1-R^2_{\bm{\gamma}})]^{(n-1)/2}}\\
\end{align*}
where $R^2_{\bm{\gamma}}$ is the ordinary coefficient of
determination, i.e.
\begin{align*}
  R^2_{\bm{\gamma}}\ &=\ 1-\frac{SSE}{SST}
\end{align*}
where $SSE$ is defined above in section \ref{fit} and $SST = (\bm{y}
-\bar{\bm{y}})'(\bm{y} -\bar{\bm{y}})$. Note that in covariate
selection context, the null model which only has an intercept has
$R^2_{\bm{\gamma}}=0$ and $k_{\bm{\gamma}}=0$. Also note that the
marginal likelihood only needs to be known up to a multiplicative
constant common to all models in order to determine Bayes factors
since the Bayes factor is a ratio of marginal likelihoods.

\subsection{Computation}

The problem, then, is computation. Given $k$ possible covariates,
there are $M=2^k$ possible regression models. As $k$ grows, the model
space quickly becomes too large to enumerate and fit all possible
regression models in a reasonable amount of time. One strategy for
dealing with this computational problem is using a gpu to fit models
simultaneously rather than sequentially on a cpu. This should
dramatically reduce computation time and thus allow for larger numbers
of covarates to be considered in the model selection procedure.

A middle step between fitting all models sequentially on a cpu and
fitting them simultaneously on a gpu is fitting them sequentially on a
gpu. That is, using a gpu to perform the linear algebra operations
necessary to fit a regression model, but fitting each model one at a
time. GPUs are uniquely adept at linear algebra operations, especially
matrix operations, so this method should result in a significant
speedup. An \verb0R0 package already exists for fitting regression
models on a GPU: \verb0gputools0. Most of my functions are based on
\verb0gputools0 in some way.

\section{Functions for Model Selection}

I created three separate functions for performing model selection
using a GPU: the \verb0R0 wrapper, the \verb0C0 wrapper, and the smart
\verb0C0 wrapper. Each of these functions is based on \verb0gputools0
in some way and as such are considered ``wrappers'' for
\verb0gputools0 functions, even if those functions end up being
somewhat modified.

\subsection{R Wrapper}

The \verb0R0 wrapper is written entirely in \verb0R0. It takes a
response vector $\bm{y}$ and a full model matrix $\bm{X}$ that
includes a column of 1's for the intercept, finds all possible
model matrices by including different columns of $\bm{X}$, then fits
these models sequentially by calling the \verb0gputools0 function
\verb0gpuLm.fit0 on $\bm{y}$ and the model matrix $\bm{X_\gamma}$. The
output is information on the top 1000 models, including AIC, BIC, log
marginal likelihood, the posterior model probability (assuming a
uniform prior over the model space), and the total posterior
probability assigned to models {\it not} included in this list.

\subsection{C Wrapper}

The \verb0C0 wrapper is essentially the same thing as the \verb0R0
wrapper, except all operations are written in \verb0C0. More
precisely, the \verb0C0 wrapper has an \verb0R0 function that takes a
response vector $\bm{y}$ and a full model matrix, including a column
of ones, $\bm{X}$, and passes it to \verb0C0. Then, a set of \verb0C0
function pull out submatrices to fit all possible models using the
\verb0gputools0 \verb0C0 function \verb0gpuLSFitF0. The output is
identical to that of the \verb0R0 wrapper.

\subsection{Smart C Wrapper}

Also known as the ``\verb0C0 smart'' or the smart wrapper, this function uses
modified CUDA \verb0C0 code from the \verb0gputools0 package. More
precisely, it does the same thing as the \verb0C0 wrapper except
instead of calling \verb0gputools0 functions, it calls modified forms
of these functions that allocate memory to the GPU in a more
intelligent fashion. Instead of reallocating memory for a model matrix
and several other necessary variables for every model, memory is
allocated once before the first model is fit and the full model matrix
is copied to the GPU. Then the current model matrix is copied from the
full matrix in a GPU-to-GPU transfer at every iteration. Several
quantities used in many or all of the models are also precomputed and
copied to the GPU so that every model has access to them during the
fitting process without having to relying on slow memory transfers
from RAM to the GPU.

\section{CUBLAS Bug}

In the process of writing these wrappers, I found a bug in the CUBLAS
library, at least in version 4.1 of the CUDA toolkit (the version
running on our machine). CUBLAS is a CUDA implementation of the BLAS
library for linear algegra. The file
\verb0~/gpuModelSearch/CublasBug/README0 contains all of the relevant
details, but the short version is that you can only open and close so
many CUBLAS contexts before the program crashes - 13,000 or so on our
system. A CUBLAS context is created in version 1 with
\verb0cublasInit();0 and destroyed with \verb0cublasShutdown();0. In
version 2 of CUBLAS it's possible to have multiple contexts running
simultaneously in the same program, so you have to name the
context. You create a context with \verb0cublasHandle_t name;0 to name
the context and \verb0cublasCreate(&name);0 to create the
context. \verb0cublasDestroy(name);0 destroys the context. It turns
out that the bug is in a function that both \verb0cublasDestroy()0 and
\verb0cublasShutdown()0 call in order to clear previously allocated
resources from the GPU.\footnote{Resrouces aren't being properly freed
  from the GPU resulting in memory fragmentation. For more details,
  see: http://www.culatools.com/blog/2012/03/12/3099/} The upshot is
that for $k > 13$, the \verb0R0 and \verb0C0 wrappers can't fit all
possible submodels before the program crashes, so we don't have timing
data on those functions for large $k$. The bug has been fixed in
version 4.2 of the CUDA toolkit and an upgrade to the toolkit on our
machine should remove this restriction, once the upgrade is available
for our OS.\footnote{For details, see
  http://developer.download.nvidia.com/compute/DevZone/docs/html/C/doc/CUDA\_Toolkit\_Release\_Notes.txt }


\section{Timing the Functions}

At the outset, we expect the \verb0C0 wrapper to be slightly faster
than the \verb0R0 wrapper and the \verb0C0 smart wrapper to be
significantly faster than both of the other wrappers. A baseline
wrapper was also created - the LM wrapper - that is essentially the
same thing as the \verb0R0 wrapper except it uses the R function
\verb0lm0 in order to fit all models sequentially on the CPU. In order
to assess how fast these functions actually are, we created full model
matrices of various sizes along with response vectors by varying the
number of observations $n$ and the number of possible covariates
$k$. These functions were fit on GPU 3 (with 0-based indexing) of the
impact1 linux server at Iowa State University. This GPU is an NVIDIA
Tesla M2070 with 5375 MB of memory and 448 CUDA Cores running CUDA
driver and runtime version 4.1. All functions were timed using the
\verb0R0 function \verb0sytem.time0 and the value \verb0elapstime0. A
wide variety of choices of $n$ and $k$ were sampled - the data is
available in the file \verb0fittime.csv0 in the folder
\verb0~/gpuModelSearch/timing0 of the github
repository.\footnote{Available online at https://github.com/jarad/gpuModelSearch}

<<sample-size, results=tex, echo=F>>=
library(ggplot2)
library(xtable)
data <- read.csv("fittime.csv")
@

First we plot the time to fit all possible models by sample size
$n$ in figure \ref{k10}, with the number of covariates ($k$)
held constant at 10. As expected, time to fit increases as the sample
size increases.  For the entire domain, it appears \verb0C0 wrapper
fits all possible models slightly faster than the \verb0R0
wrapper while the LM wrapper is signicantly faster than both. This is
rather suprising - the overhead required to work on the GPU outweighs
any speedup gains compared to a naive sequential algorithm using
\verb0lm0. The smart \verb0C0 wrapper removes much of this overhead
and as a result itappears to be significantly faster than all of the
wrappers and the difference in speed appears to increase as the number
of observations, $n$, increases.

\begin{figure}[ht]
  \centering
<<k10, fig.width=4, fig.height=4, out.width='.7\\textwidth', echo=F>>=
mns <- aggregate(data[,3:5], list(n=data$n,k=data$k,wrap=data$wrap), "mean")
qplot(data=mns[mns$k==10 & mns$n <= 2000000,],x=n, y=elapstime, color=wrap, geom="line", log="x") #$
@
\caption{Mean time to fit over 5 samples (in seconds) vs. $n$ (log
  scale) by wrapper type.''R'' denotes the R wrapper, ``C'' denotes
  the C wrapper, ``CS'' denotes the smart C wrapper and ``LM'' denotes
  the non-GPU LM wrapper.}
\label{k10}
\end{figure}

Plotting the ratios of times to fit in figure \ref{k10rat} adds more
information to the story. The \verb0C0 and \verb0R0 wrappers are
significantly slower than the LM wrapper, especially for small
$n$. For $n>10,000$, they are roughly half as fast as the LM wrapper
and basically indistinguishable from each other, though the \verb0C0
wrapper appears to be slightly faster over most of the domain. For
small $n$, the smart wrapper is also significantly slower than the LM
wrapper, but as $n$ increases beyond 1000, the mean ratio of LM fit
time to CS fit time increases approximately linearly so that at
$n=2,000,000$ the smart wrapper is about 4 times faster than the
non-GPU LM wrapper. At least for $k=10$, using a GPU is only faster
for $n>10,000$ or so.

\begin{figure}[ht]
  \centering
<<k10rat, fig.width=4, fig.height=4, out.width='.7\\textwidth', echo=F>>=
rdat <- data[data$wrap=="R",-6]
cdat <- data[data$wrap=="C",-6]
csdat <- data[data$wrap=="CS" & data$n<=2000000 & data$k<=13,-6]
lmdat <- data[data$wrap=="LM" & data$n<=2000000 & data$k<=13, -6]
rratio <- rdat
rratio[,3:5] <- lmdat[,3:5] / rdat[,3:5]
cratio <- cdat
cratio[,3:5] <- lmdat[,3:5] / cdat[,3:5]
csratio <- csdat
csratio[,3:5] <- lmdat[,3:5] / csdat[,3:5]

rratmns <- aggregate(rratio[,3:5], list(n=rratio$n,k=rratio$k), "mean")
cratmns <- aggregate(cratio[,3:5], list(n=cratio$n,k=cratio$k), "mean")
csratmns <- aggregate(csratio[,3:5], list(n=csratio$n,k=csratio$k), "mean")

colnames(rratmns)[3:5] <- paste(c("usr","sys","elaps"), "ratio", sep="")
colnames(cratmns)[3:5] <- paste(c("usr","sys","elaps"), "ratio", sep="")
colnames(csratmns)[3:5] <- paste(c("usr","sys","elaps"), "ratio", sep="")

rratmns$ratio <- "LM / R"
cratmns$ratio <- "LM / C"
csratmns$ratio <- "LM / CS"

ratmns <- rbind(rratmns, cratmns, csratmns)

qplot(data=ratmns[ratmns$k==10,],x=n, y=elapsratio, color=ratio, geom="line", log="x")
@
\caption{Ratio of LM fit time to various wrapper fit times vs. n (log scale) for
  $k=10$. Ratios are calculated as, e.g. for "LM / R",  mean(fit time
  for LM wrapper / fit  time for R wrapper).}
\label{k10rat}
\end{figure}


The data for $k=5$ tell a different story in figure \ref{k5}: the
\verb0C0 wrapper is slightly faster than the \verb0R0 wrapper at small
sample sizes, but as $n$ increases the R wrapper eventually becomes
faster. The difference is small though; the larger disparity is once
again between smart wrapper and both the \verb0C0 and \verb0R0
wrappers. The smart wrapper appears significantly faster, especially
as $n$ increases. The LM wrapper's speed is, once again, about midway
between the \verb0R0 and \verb0C0 wrappers at the slow end and the
smart wrapper at the fast end. Figure \ref{k5rat} shows the same
phenomenon in terms of ratios. The \verb0R0 and \verb0C0 wrappers are
roughly as fast as each other with the \verb0C0 wrapper faster over
most of the domain, though for very large $n$ the \verb0C0 wrapper is
surprisingly {\it slower} than the \verb0R0 wrapper. For large $n$,
their speed stabilizes at roughly half that of the non-GPU LM wrapper.
The trajectory of the relative speed of the smart wrapper is a bit
different in this case. For small $n$, once again LM is faster and
once again starting at $n=1000$ or so, the relative speed of CS to LM
is increasing linearly. However, at $n=100,000$ it levels off with CS
about 2.5 times faster than LM. It appears that larger $n$ doesn't
necessarily mean better speed gains from using the GPU.

\begin{figure}[ht]
  \centering
<<k5, fig.width=4, fig.height=4, out.width='.7\\textwidth', echo=F>>=
qplot(data=mns[mns$k==5 & mns$n<=2000000,],x=n, y=elapstime, color=wrap, geom="line", log="x")
@
\caption{Time to fit (in seconds) vs. $n$ (log scale) by wrapper type,
  $k=5$.}
\label{k5}
\end{figure}

\begin{figure}[ht]
  \centering
<<k5rat, fig.width=4, fig.height=4, out.width='.7\\textwidth', echo=F>>=
qplot(data=ratmns[ratmns$k==5,],x=n, y=elapsratio, color=ratio, geom="line", log="x", ylim=c(0,8)) #$
@
\caption{Ratio of wrapper fit times vs. $n$ (log scale) for $k=5$.}
\label{k5rat}
\end{figure}

Figure \ref{n100} tells an interesting story when $n$ is held constant
at 100 and $k$ increases. The \verb0C0 wrapper once again is slightly
faster than the \verb0R0 wrapper while the smart wrapper is faster
than both, though not nearly as significantly so in the range of $k$
sampled. However, the non-GPU LM wrapper is much faster than any of
the GPU wrappers for larger $k$. The ratios show in figure
\ref{n100rat} that all of the GPU wrappers, as $k$ increases the LM
wrapper becomes relatively faster. The smart wrapper is the fastest of
the GPU wrappers, but except for $k=5$ it's still less than a tenth as
fast as the LM wrapper. Taken with the previous plots, this plot
suggests that the smart wrapper is only advantaged when $n>>k$. As the
two become relatively close, it appears that the overhead associated
with making transfers to and from the GPU dominate any speedup in the
linear algebra operations.

\begin{figure}[ht]
  \centering
<<n100, fig.width=4, fig.height=4, out.width='.7\\textwidth', echo=F>>=
qplot(data=mns[mns$n==100 & mns$k<=13,],x=k, y=elapstime, color=wrap, geom="line")
@
\caption{Time to fit (in seconds) vs. $k$ by wrapper type
  $n=100$. Recall that due to a bug in the CUBLAS library, $k$ must be
  kept small for all wrappers except the smart wrapper.}
\label{n100}
\end{figure}
\begin{figure}[ht]
  \centering
<<n100rat, fig.width=4, fig.height=4, out.width='.7\\textwidth', echo=F>>=
qplot(data=ratmns[ratmns$n==100,],x=k, y=elapsratio, color=ratio, geom="line")
@
\caption{Ratio wrapper fit times vs. $k$ for $n=100$.}
\label{n100rat}
\end{figure}

Finally, we take a look at the absolute fit time of the smart wrapper
in order to estimate how long it would take to fit all possible models
with a wider range of $n$ and $k$. In order to assess this, we sampled
a wider range of $n$ and $k$ for the smart wrapper and fit the
regression model:
\[
\mathrm{log}(elapstime_i)\stackrel{iid}{\sim}\mathrm{N}(\beta_0+n_i\beta_n+k_i\beta_k, \sigma^2)
\]
The results of the fit are below.
<<csREG, echo=F>>=
csdata <- data[data$wrap=="CS",]
cs.log.model <- lm(log(elapstime)~n+k, data=csdata)
summary(cs.log.model)
@
Unsuprisingly, both $n$ and $k$ are important predictors of log time
to fit and $k$'s effect is several orders of magnitude larger. Based
on this regression, table \ref{CSreg} contains predicted fit times for
a wide range of values for $n$ and $k$. For $n<=10,000$ essentially
the only determinant of fit time is $k$. At $k=30$, the maximum number
of covariates for which the table contains a prediction, the time to
fit is relatively stable at 94--96 days for values of $n$ less than
10,000 -- note that for $k=30$ there are $2^{30}$ or approximately
1,000,000,000 (one billion) regression models to fit. Similarly for
$k=25$, the time to fit is around 2.85 days for $n$ less than
10,000. For $k>25$ or so, the time to fit all possible models becomes
relatively prohibitive no matter the number of observations.

At $n=100,000$, fit times start increasing exponentially with $n$
across all values of $k$.  At $n=1,000,000$, smaller values of $k$
become prohibitive to fit, and this trend continues for $n=3,000,000$
and larger. At $n=5,000,000$, it takes over 5 days to fit all possible
models with just 10 possible covariates and at $n=7,000,000$ it would
take over a year.



<<csTable, results=tex, echo=F>>=
ns <- c(100, 1000, 10000, 100000, 1000000, 3000000, 5000000, 7000000)
ks <- 10:30
nk <- length(ks)
nn <- length(ns)
n <- nk*nn

X <- cbind(rep(1, n), rep(ns, nk), c(kronecker(ks, rep(1,nn))))
beta <- t(t(coef(cs.log.model)))
est <- X%*%beta
est.mat <- matrix(est, ncol=nn, byrow=T)
rownames(est.mat) <- paste("k=", ks, sep="")
colnames(est.mat) <- paste("n=", ns, sep="")
est.day.mat <- exp(est.mat)/60/60/24
xtable(est.day.mat, caption="Predicted time in days to fit all possible regression models using the C smart wrapper, based on a regression of log(elapstime) on n and k.", label="CSreg")
@
\section{Further Work}

\subsection{Iterative Simple Linear Regression}

One idea to avoid doing matrix arithmetic in the model selection
algorithm, suggested by Dan Nettleton, is to avoid computing the
coefficient estimates altoether. In order to do this, we first fit all
simple linear regression models. Then to fit regression models with
two variables included, we find one of the SLR models with only one of
the two desired covariate then regress the new variable on the old
variable. Then we regress the residuals of the original regression
model on the residuals of the new covariate regression model, yielding
the residuals of the larger model. In this fashion, we can start from
the smallest possible models and iteratively obtain the residuals of
every possible model using only simple linear regressions -
i.e. avoiding all matrix calculations. \verb0sanitycheck.r0 provides
an example. The next subsection fleshes out how the process is supposed
to work and the following subsection is a bit of theory showing that
it does work.
\subsection{Iterative Simple Linear Regression}

One idea to avoid doing matrix arithmetic in the model selection
algorithm, suggested by Dan Nettleton, is to avoid computing the
coefficient estimates altoether. In order to do this, we first fit all
simple linear regression models. Then to fit regression models with
two variables included, we find one of the SLR models with only one of
the two desired covariate then regress the new variable on the old
variable. Then we regress the residuals of the original regression
model on the residuals of the new covariate regression model, yielding
the residuals of the larger model. In this fashion, we can start from
the smallest possible models and iteratively obtain the residuals of
every possible model using only simple linear regressions -
i.e. avoiding all matrix calculations. \verb0sanitycheck.r0 provides
an example. The next subsection fleshes out how the process is supposed
to work and the following subsection is a bit of theory showing that
it does work.

\subsubsection{The Iterative SLR Process}\label{addedvariable}

Suppose we want to obtain the residuals from a full regression model
\begin{align}\label{full}
\bm{y} = \bm{X}\bm{\beta} + \bm{\epsilon}
\end{align}
without actually fitting the model. Break up $\bm{X}$ and $\bm{\beta}$
into
\[
\bm{X} = \begin{bmatrix} \bm{X_1}, & \bm{X_2} \\\end{bmatrix}
\ and\
\bm{\beta} = \begin{bmatrix} \bm{\beta_1} \\ \bm{\beta_2} \end{bmatrix}
\]
so that
\[
\bm{y} = \bm{X_1}\bm{\beta_1} + \bm{X_2}\bm{\beta_2} + \bm{\epsilon}
\]

Now suppose we have already fit the model
\begin{align}\label{red}
\bm{y} = \bm{X_1}\bm{\beta_1}+\bm{\delta}_{red}
\end{align}
and have at our disposal its estimated residuals
$\hat{\bm{\delta}}_{red}$. Also suppose that $\bm{X_2}$ is a single column
vector. Then fit the added variable regression
\begin{align}\label{add}
\bm{X_2} = \bm{X_1}\bm{\lambda_1}+\bm{\delta}_{add}
\end{align}
Then if fit the residual regression
\begin{align}\label{residreg}
\hat{\bm{\delta}}_{red} = \hat{\bm{\delta}}_{add}\gamma + \bm{\eta}
\end{align}
the estimate residuals of the residual regression are equal to that of
the full model, that is $\hat{\bm{\eta}} = \hat{\bm{\epsilon}}$. So as
long as we can obtain $\hat{\bm{\delta}}_{add}$ cheaply, we don't have
to do matrix algebra in order to obtain $\hat{\bm{\epsilon}}$. Note
that once the residuals for the full model are obtained, the full
model can now be the reduced model so that another column can be added
to the design matrix. The only wrinkle here is that obtaining
$\hat{\bm{\delta}}_{add}$ without doing matrix operations may not be
possible when $\bm{X_1}$ contains many columns, thouogh I haven't
looked into this.

It's worth noting that this process doesn't seem like it would work
that well with a model search algorithm since the search algorithm
would try to fit the best models while this algorithm naturally fits
all models by first fitting the smallest, then iteratively fitting
larger models.



\subsubsection{Residual Relationship}
I'll show why this basic process works here. First, some
preliminaries. Suppose we have the following regression model
\[
\bm{y} = \bm{X}\bm{\beta} + \bm{\epsilon}
\]
The least squares estimate for $\bm{\beta}$ is
\[
\bm{\hat{\beta}} = \left(\bm{X}'\bm{X}\right)^{-1}\bm{X}'\bm{y}
\]
which yields the estimated residuals
\[
\bm{\hat{\epsilon}} = \bm{y} - \bm{\hat{y}} = \bm{y} -
\bm{X}\bm{\hat{\beta}} = \bm{y} -
\bm{X}\left(\bm{X}'\bm{X}\right)^{-1}\bm{X}'\bm{y} = \bm{y} -
\bm{P_X}\bm{y} = \left(\bm{I} - \bm{P_X}\right)\bm{y}
\]
where $\bm{P_X} = \bm{X}\left(\bm{X}'\bm{X}\right)^{-1}\bm{X}'$ is the
projection matrix into the column space of $\bm{X}$.

So the estimated residuals for model \eqref{full} -- the full model -- are
\[
\bm{\hat{\epsilon}} = (\bm{I} - \bm{P_X})\bm{y}
\]
the estimated residuals for model \eqref{red} -- the
reduced model -- are
\[
\bm{\hat{\delta}}_{red} = (\bm{I} - \bm{P_{X_1}})\bm{y}
\]
the estimated residuals for model \eqref{add} -- the added variable -- are
\[
\bm{\hat{\delta}}_{add} = (\bm{I} - \bm{P_{X_1}})\bm{X_2}
\]
and the estimated residuals for model \eqref{residreg} -- the residual
regression model -- are
\begin{align}
\bm{\hat{\eta}} &= [\bm{I} - \hat{\bm{\delta}}_{add}
(\hat{\bm{\delta}}_{add}'\hat{\bm{\delta}}_{add})^{-1}
\hat{\bm{\delta}}_{add}']\hat{\bm{\delta}}_{red}\\
&= \left[\bm{I} - (\bm{I} - \bm{P_{X_1}})\bm{X_2} [\bm{X_2}'((\bm{I} -
  \bm{P_{X_1}})'(\bm{I} - \bm{P_{X_1}})\bm{X_2}]^{-1} \bm{X_2}'(\bm{I}
  - \bm{P_{X_1}}) \right] (\bm{I} - \bm{P_{X_1}})\bm{y}\\
& = \hat{\bm{\delta}}_{red} - (\bm{I} -
\bm{P_{X_1}})\bm{X_2}[\bm{X_2}'(\bm{I} -
\bm{P_{X_1}})\bm{X_2}]^{-1}\bm{X_2}'(\bm{I}-\bm{P_{X_!}})\hat{\bm{\delta}}_{red} \label{eta}
\end{align}


In order for the process outlined in the section \ref{addedvariable} to work, we need
$\bm{\hat{\epsilon}} = \bm{\hat{\eta}}$, which I'll now show.

Start with the full model with the model matrix split up
\[
\bm{y} = \begin{bmatrix} \bm{X_1}, &
  \bm{X_2} \end{bmatrix} \begin{bmatrix} \bm{\beta_1} \\
  \bm{\beta_2} \end{bmatrix} + \bm{\epsilon}
\]
The we can rewrite $\bm{\hat{\beta}}$ as
\begin{align}\label{betahat}
\bm{\hat{\beta}} = \left(\begin{bmatrix} \bm{X_1}' \\
    \bm{X_2}' \end{bmatrix} \begin{bmatrix}
  \bm{X_1}, & \bm{X_2} \end{bmatrix} \right)^{-1} \begin{bmatrix}
\bm{X_1}' \\ \bm{X_2}' \end{bmatrix}\bm{y}
= \begin{bmatrix} \bm{X_1}'\bm{X_1} & \bm{X_1}'\bm{X_2} \\
  \bm{X_2}'\bm{X_1} & \bm{X_2}'\bm{X_2} \end{bmatrix}^{-1}
\begin{bmatrix} \bm{X_1}'\bm{y} \\ \bm{X_2}'\bm{y} \end{bmatrix}
\end{align}

We can find the inverse in \eqref{betahat} using the following known formula:
\[
\begin{bmatrix} \bm{A} & \bm{B} \\ \bm{C} & \bm{D} \end{bmatrix} ^{-1} =
\begin{bmatrix} \bm{A}^{-1} + \bm{A}^{-1}\bm{B}\bm{S}\bm{C}\bm{A}_{-1}
  & -\bm{A}^{-1}\bm{B}\bm{S} \\
  -\bm{S}\bm{C}\bm{A}^{-1} & \bm{S} \end{bmatrix}
\]
where $\bm{S} = (\bm{D} - \bm{C}\bm{A}^{-1}\bm{B})^{-1}$ is the
inverse Schur complement of $\bm{A}$. In this case
\[
\bm{S} = \left(\bm{X_2}'\bm{X_2} -
  \bm{X_2}'\bm{X_1}(\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}'\bm{X_2}\right)^{-1}
= (\bm{X_2}'\bm{X_2} - \bm{X_2}'\bm{P_{X_1}}\bm{X_2})^{-1} =
\left[\bm{X_2}'(\bm{I} - \bm{P_{X_1}})\bm{X_2}\right]^{-1}
\]
and
\begin{align*}
\bm{\hat{\beta}} &= \widehat{\begin{bmatrix} \bm{\beta_1} \\
    \bm{\beta_2} \end{bmatrix}} =
\begin{bmatrix} (\bm{X_1}'\bm{X_1})^{-1} +
  (\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}'\bm{X_2} \bm{S}
  \bm{X_2}'\bm{X_1}(\bm{X_1}'\bm{X_1})^{-1} &
  -(\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}'\bm{X_2}\bm{S} \\
  -\bm{S}\bm{X_2}'\bm{X_1}(\bm{X_1}'\bm{X_1})^{-1} &
  \bm{S} \end{bmatrix}
\begin{bmatrix} \bm{X_1}'\bm{y}\\ \bm{X_2}'\bm{y}\end{bmatrix} \\
&= \begin{bmatrix} (\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}\bm{y} +
  (\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}'\bm{X_2}\bm{S}\bm{X_2}'\bm{P_{X_1}}\bm{y}
  - (\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}'\bm{X_2}\bm{S}\bm{X_2}'\bm{y} \\
  -\bm{S}\bm{X_2}'\bm{P_{X_1}}\bm{y} +
    \bm{S}\bm{X_2}'\bm{y} \end{bmatrix}\\
&=\begin{bmatrix} (\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}'\bm{y} -
  (\bm{X_1}'\bm{X_1})^{-1}\bm{X_1}'\bm{X_2}\bm{S}\bm{X_2}'\bm{\hat{\delta}}_{red}\\
  \bm{S}\bm{X_2}'\bm{\hat{\delta}}_{red} \end{bmatrix}
\end{align*}
This yields
\begin{align*}
\bm{\hat{y}} &= \bm{X}\bm{\hat{\beta}} = \begin{bmatrix} \bm{X_1} &
  \bm{X_2} \end{bmatrix} \bm{\hat{\beta}} = \bm{P_{X_1}}\bm{y} -
\bm{P_{X_1}}\bm{X_2}\bm{S}\bm{X_2}'\bm{\hat{\delta}}_{red} +
\bm{X_2}\bm{S}\bm{X_2}'\bm{\hat{\delta}}_{red} \\
&= \bm{P_{X_1}}\bm{y} +
(\bm{I}-\bm{P_{X_1}})\bm{X_2}\bm{S}\bm{X_2}'\bm{\hat{\delta}}_{red}
\end{align*}
So
\begin{align*}
\bm{\hat{\epsilon}} &= \bm{y}-\bm{\hat{y}} = \bm{\hat{\delta}}_{red} -
(\bm{I}-\bm{P_{X_1}})\bm{X_2}\left[\bm{X_2}'(\bm{I}-\bm{P_{X_1}})
  \bm{X_2}\right]^{-1}\bm{X_2}'\bm{\hat{\delta}}_{red}\\
&=\hat{\bm{\eta}}
\end{align*}

\subsection{Model Per Block}

Short of a clever algorithm like the one proposed in the previous
section, there are two places to speed up the model fitting
computations and thus model selection computations. The first is in
the linear algebra required to fit a model. The \verb0R0 function
\verb0lm0 finds $\hat{\bm{\beta}}$ using a Householder transformation
method. Following this, the \verb0gputools0 function \verb0gpuLSFitF0
uses a blocked Householder transormation method that is optimized for
accessing GPU memory. This process dramatically speeds up the linear
algebra required in the fitting process. The second place where speed
ups can occur is by fitting models simultaneously on the GPU rather
than sequentially. It's likely the case that the most dramatic speed
ups will occur from combining these two methods.

A natural way to accomplish this is to assign each submodel a block of
threads. Threads are organized into blocks that can easily communicate
with each other on the GPU. Threads from different blocks, however,
can't communicate as easily. By assigning each block a model, the
linear algebra operations required to fit each model still be
parallelized among the threads within that block but the GPU can work
on separate blocks and thus separate models simultaneously.

In order to implement this procedure, a whole suite of linear algebra
functions has to be written. NVIDIA created the CUBLAS library, a
GPU implementations of BLAS, but it's a set of GPU kernels that are
only callable from inside a CPU process. Many of these functions will
have to essentially be rewritten to be callable from within a block of
threads so that the linear algebra required to fit models can be
parallelized.

\subsection{Model Per Thread}

There is a lot of overhead associated with passing information to and
from the GPU and across threads or blocks of threads within the
GPU. It's possible that this overhead outweighs any gain achieved by
parallelizing the linear alebra in the model fitting process. A
strategy that would avoid that overhead is to assign each thread it's
own model and fit that model using sequential linear algebra. Once
again, a whole suite of BLAS functions will have to be written so that
they're callable on a thread of a GPU.

\subsection{Hybrid}

So there are three strategies for speeding of the fitting process of
linear models. They are

\begin{enumerate}
\item Parallelize the linear algebra. Implemented by \verb0gputools0
  and all of my wrappers.
\item Fit models simultaneously.
\item Fit models simultaneously while also parallelizing the linear algebra.
\end{enumerate}

If it weren't for overhead, 3 would clearly be the fastest. As it
stands, we're uncertain about which strategy will be faster. It's
plausible that different strategies are faster in different cases. If
this is true and some feature of the particular submodel we are
fitting gives us useful information about which strategy will be
faster, it may be possible to construct a hybrid algorithm that picks
for each submodel the fastest fitting method based on some easily
computed criteria. Ideally it would depend on something known a priori
like $n$, $k$ or a simple function thereof.

\bibliography{gpubib}
\bibliographystyle{plainnat}

\end{document}
